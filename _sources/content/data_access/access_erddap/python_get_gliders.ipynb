{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31799af2",
   "metadata": {},
   "source": [
    "Batch access (ex: gliders)\n",
    "===\n",
    "## Packages used\n",
    "To use python to access the ERDDAP server directly from your python script or jupyter-notebook, you will need\n",
    "- ERDDAPY\n",
    "- Xarray\n",
    "- netcdf4 \n",
    "- matplotlib\n",
    "- folium\n",
    "- numpy\n",
    "\n",
    "```{note}\n",
    "The package [**netcdf4**](http://unidata.github.io/netcdf4-python/) develop by UNIDATA is not needed in the import part of the python script. However, it is the essential package that [support netCDF format output from Xarray](https://docs.xarray.dev/en/stable/user-guide/io.html). The package [**matplotlib**](https://matplotlib.org/stable/) is also not needed in the import part of the python script. It is the essential package that [support quick visualization from Xarray](https://docs.xarray.dev/en/stable/user-guide/plotting.html). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe25af",
   "metadata": {},
   "source": [
    "In this page, we demonstrate how to extract/download data directly from a ERDDAP server and perform data processing, visualization, and export data in python environment. \n",
    "\n",
    "```{tip}\n",
    "[Understanding of the ERDDAP server and what it provides](erddapData) is highly recommended before reading the following intructions.\n",
    "```\n",
    "\n",
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1daed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from erddapy import ERDDAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9ff39",
   "metadata": {},
   "source": [
    "- [**xarray**](https://docs.xarray.dev/en/stable/getting-started-guide/why-xarray.html) is used for data processing and netCDF file output. \n",
    "- [**erddapy**](https://ioos.github.io/erddapy/00-quick_intro-output.html) is used to access the ERDDAP server.\n",
    "\n",
    "Both package-webpages have more detail explanation on its full range of functionalities. \n",
    "Here we will mainly focusing on getting the data to be displayer and downloaded.\n",
    "\n",
    "\n",
    "## Access IOOS Glider DAC (TableDAP) data\n",
    "In this demostration, we will be getting the glider data from [IOOS Glider Data Archived Center](https://gliders.ioos.us/erddap/tabledap/allDatasets.html)\n",
    "\n",
    "Firstly, the way to use the **erddapy** is to setup the destination ERDDAP server as an object in python through `ERDDAP` ([a python class](https://docs.python.org/3/tutorial/classes.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### access the ERDDAP server \n",
    "e = ERDDAP(\n",
    "    server=\"https://gliders.ioos.us/erddap/\",             # The URL that the ERDDAP server has\n",
    "    protocol=\"tabledap\",                                  # The data type (griddap or tabledap)\n",
    "    response=\"opendap\",                                   # different output data type that provided by ERDDAP server       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4111d",
   "metadata": {},
   "source": [
    "By executing the above code block, we have already setup the connection with the desired ERDDAP server. \n",
    "To request a the dataset list on the server, we need to set the `dataset_id = allDatasets`(e.g. https://gliders.ioos.us/erddap/tabledap/allDatasets.html).\n",
    "\n",
    "To set the `dataset_id`, execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceef394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dataset id name \n",
    "e.dataset_id = \"allDatasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f72b7",
   "metadata": {},
   "source": [
    "## Download data list\n",
    "Now, all the setting for downloading the dataset list is complete. By executing the following, we convert the list into a pandas dataframe object that contain all the dataset info on the ERDDAP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e562e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = e.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce870b1c",
   "metadata": {},
   "source": [
    "Let's take a peak at the first five glider deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3298d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4b351",
   "metadata": {},
   "source": [
    "Now, let us create a list of dataset_id. This helps if user want to download all datasets on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = []\n",
    "for dataset_id in df['datasetID']:\n",
    "    if dataset_id != 'allDatasets':\n",
    "        dataset_ids.append(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cb8f7",
   "metadata": {},
   "source": [
    "The first five `dataset_id` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ae6fb",
   "metadata": {},
   "source": [
    "## Picking one dataset to investigate on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.dataset_id = 'ce_311-20190703T1802-delayed'    # feel free to uncomment and test\n",
    "# e.dataset_id = 'amelia-20201015T1436'            \n",
    "e.dataset_id = 'UW685-20230125T0000'             # feel free to uncomment and test\n",
    "df = e.to_pandas()\n",
    "ds = df.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de41085",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here, we try to extract the data from the server by first converting it into a Pandas dataframe then into a Xarray dataset. A direct `e.to_xarray()` would result in another data structure which is harder to visualize in this notebook.\n",
    "\n",
    "Converting from the dataframe to the xarray helps us to convert into a gridded dataset in the later on preprocessing step. But if that is not needed, `ds = df.to_xarray()` is not a necessary step to extract the dataset from the server.\n",
    "```\n",
    "\n",
    "\n",
    "### Preprocessing the data\n",
    "To focus on the vertical profile, we first remove row that does not contain the depth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8512e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_transect = ds.where(ds['depth (m)'].notnull(),drop=True)   # only preserve the profile related data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495653e9",
   "metadata": {},
   "source": [
    "The original dataset structure is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059daa8",
   "metadata": {},
   "source": [
    "After the `.where` filtering, the dataset structure becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273316a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_transect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb6129",
   "metadata": {},
   "source": [
    "This imidiately decrease the table from 90984 rows to 39661 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af13e06",
   "metadata": {},
   "source": [
    "### Creating 3D plotting function\n",
    "To take a quick look on the glider vertical profile, we construct a plotting function using the matplotlib package which could avoid rewriting the plotting code over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_3d_view(ele_angle=-140,hori_angle=60):\n",
    "    \"\"\"\n",
    "    This function is designed for creating axe object) for plotting the \n",
    "    3D scatter plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ele_angle : integer\n",
    "        elevation angle of viewing the 3D scatter plot\n",
    "    hori_angle : integer\n",
    "        horizontal angle of viewing the 3D scatter plot\n",
    "    Returns\n",
    "    -------\n",
    "    ax : matplotlib axe object\n",
    "        the axe object that can apply scatter3D method \n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = plt.axes([0,0,1.5,1],projection = '3d')\n",
    "    ax.view_init(ele_angle, hori_angle)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Depth')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197726f",
   "metadata": {},
   "source": [
    "### Plotting a subset of the dataset and focusing on single variables\n",
    "- temperature profile along the glider path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_part = ds_transect\n",
    "varname = 'temperature (Celsius)'\n",
    "zname = 'depth (m)'\n",
    "yname = 'latitude (degrees_north)'\n",
    "xname = 'longitude (degrees_east)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df44dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plot_3d_view(ele_angle=-140,hori_angle=60)\n",
    "p = ax2.scatter3D(ds_part[xname],\n",
    "                  ds_part[yname],\n",
    "                  ds_part[zname],\n",
    "                  c=ds_part[varname],      # color value of individual points is taken from their heights\n",
    "                  cmap=\"viridis\"                           # the color mapping to be used. Other example options: winter, autumn, ...\n",
    "                )\n",
    "# ax2.invert_zaxis()\n",
    "ax2.invert_xaxis()\n",
    "cbar = plt.colorbar(p)\n",
    "cbar.set_label(varname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0bcdf",
   "metadata": {},
   "source": [
    "- salinity profile along the glider path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'salinity (0.001)'\n",
    "zname = 'depth (m)'\n",
    "yname = 'latitude (degrees_north)'\n",
    "xname = 'longitude (degrees_east)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = plot_3d_view(ele_angle=-160,hori_angle=110)\n",
    "p = ax2.scatter3D(ds_part[xname],\n",
    "                  ds_part[yname],\n",
    "                  ds_part[zname],\n",
    "                  c=ds_part[varname],      # color value of individual points is taken from their heights\n",
    "                  cmap=\"viridis\"                           # the color mapping to be used. Other example options: winter, autumn, ...\n",
    "                )\n",
    "# ax2.invert_zaxis()\n",
    "ax2.invert_xaxis()\n",
    "cbar = plt.colorbar(p)\n",
    "cbar.set_label(varname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32356c31",
   "metadata": {},
   "source": [
    "### Showing the geopgraphic location of the glider path\n",
    "\n",
    "Using the FOLIUM package, we can plot a interactive map in the notebook environment to investigate the path of the glider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "\n",
    "lon = ds_part[xname].data\n",
    "lat = ds_part[yname].data\n",
    "\n",
    "fmap = folium.Map(location=[(np.min(lat)+np.max(lat))/2, (np.min(lon)+np.max(lon))/2], tiles=\"OpenStreetMap\", zoom_start=8)\n",
    "points = [[lat[i],lon[i]] for i in range(len(lon)) ]\n",
    "folium.PolyLine(points, color='red', weight=2.5, opacity=0.4,popup=f'{dataset_id}').add_to(fmap)\n",
    "folium.Marker([lat[0],lon[0]], popup=f'start').add_to(fmap)\n",
    "folium.Marker([lat[-1],lon[-1]], popup=f'end').add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664a1d9",
   "metadata": {},
   "source": [
    "## Converting the table data to gridded data\n",
    "\n",
    "In the following steps, we demostrate how to convert a point observation from glider to a gridded data.\n",
    "The gridded dataset will have two dimensions - time as first dimension and depth as the second dimension.\n",
    "\n",
    "\n",
    "By using the `numpy.unique` method, we first establish the two dimensions and the assoicated arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# idealy/theoretically the three array should have same len (first dimension)\n",
    "# lat = np.unique(ds_transect['latitude (degrees_north)'].data)\n",
    "# lon = np.unique(ds_transect['longitude (degrees_east)'].data)\n",
    "time = np.unique(ds_transect['time (UTC)'].data)\n",
    "dtime = pd.to_datetime(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical profile (second dimension)\n",
    "depth = np.unique(ds_transect['depth (m)'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e205d1",
   "metadata": {},
   "source": [
    "Then, we create an empty `xr.DataArray` to later put the interpolated values into the corresponding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58283668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2D gridded array for vertical profile along the glider trajectory\n",
    "da_transect = xr.DataArray(\n",
    "    coords={\n",
    "        \"depth\": depth,\n",
    "        \"time\": dtime.values,\n",
    "        # \"lon\": (\"time\", lon),  # place holder not real lon at the associated time\n",
    "        # \"lat\": (\"time\", lat),  # place holder not real lat at the associated time\n",
    "    },\n",
    "    dims=[\"depth\", \"time\"],\n",
    ")\n",
    "da_transect = da_transect.rename('var')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224a5b5",
   "metadata": {},
   "source": [
    "To convert the table data into gridded data, we perform the following processes\n",
    "1. for each single time profiling, we take all the vertical measurement\n",
    "2. for each single time profiling, we average the measurement on the same vertical level\n",
    "3. if multiple location is registered during the vertical profiling, we use the first recorded location as the profiling location\n",
    "4. the veritical profile is linearly interpolated to make every vertical profiling to have the same number of grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070f15d",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# we minimized the number of time profiling (first 10 time stamp) that needed to be processed to gridded data in this notebook (efficiency)\n",
    "print(f'processing {varname}')\n",
    "for i in range(len(time[:10])):\n",
    "    print(time[i])\n",
    "    loc_time = time[i]\n",
    "    \n",
    "    # only preserve single time (mid point time during the profiling)\n",
    "    ds_temp = ds_transect.where((ds['time (UTC)'] == loc_time),drop=True)\n",
    "    ds_temp = ds_temp.assign_coords({'depth':ds_temp['depth (m)']})\n",
    "    \n",
    "    # group mean the value for up and down profiling (single depth single value)\n",
    "    ds_temp = ds_temp.groupby('depth (m)').mean()\n",
    "    \n",
    "    # create profile data array to store profiling at one location\n",
    "    da_var = xr.DataArray(\n",
    "        ds_temp[varname].data,\n",
    "        coords={\"depth\": ds_temp['depth (m)'].data},\n",
    "        dims=[\"depth\"]\n",
    "    )\n",
    "    da_var = da_var.rename('var')\n",
    "\n",
    "    # put the single profiling to the 2D gridded dataarray\n",
    "    da_var = da_var.interp(\n",
    "        depth=depth,\n",
    "        method=\"linear\",\n",
    "        kwargs={\"fill_value\": np.nan},\n",
    "    )\n",
    "    da_transect[:,i] = da_var "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f60416",
   "metadata": {},
   "source": [
    "The gridded dataset that has the interpolated values put in is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1399da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_transect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd4b83",
   "metadata": {},
   "source": [
    "## Plotting the gridded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920386e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_transect.where(da_transect.notnull(),drop=True).plot(yincrease=False, x='time', y='depth')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   29,
   39,
   42,
   56,
   63,
   70,
   73,
   78,
   80,
   84,
   86,
   90,
   95,
   99,
   101,
   105,
   111,
   122,
   124,
   126,
   128,
   130,
   132,
   136,
   141,
   170,
   175,
   183,
   195,
   199,
   206,
   218,
   224,
   237,
   246,
   256,
   259,
   262,
   274,
   282,
   314,
   317,
   319,
   323
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}